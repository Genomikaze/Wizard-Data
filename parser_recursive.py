import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
import datetime
import time
import random
import sys

parsed_urls = set()

user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:113.0) Gecko/20100101 Firefox/113.0",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.71 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/18.19041",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:118.0) Gecko/20100101 Firefox/118.0"
]

def get_headers():
    return {
        "User-Agent": random.choice(user_agents)
    }

def format_date(date_str):
    months = {
        "—è–Ω–≤–∞—Ä—è": "01", "—Ñ–µ–≤—Ä–∞–ª—è": "02", "–º–∞—Ä—Ç–∞": "03", "–∞–ø—Ä–µ–ª—è": "04",
        "–º–∞—è": "05", "–∏—é–Ω—è": "06", "–∏—é–ª—è": "07", "–∞–≤–≥—É—Å—Ç–∞": "08",
        "—Å–µ–Ω—Ç—è–±—Ä—è": "09", "–æ–∫—Ç—è–±—Ä—è": "10", "–Ω–æ—è–±—Ä—è": "11", "–¥–µ–∫–∞–±—Ä—è": "12"
    }
    try:
        date_str = date_str.replace("–≥–æ–¥–∞", "").strip()
        day, month_str, year = date_str.split()
        month = months.get(month_str.lower(), "01")
        return datetime.date(int(year), int(month), int(day)).strftime("%Y-%m-%d")
    except Exception:
        return "–ù–µ–≤–µ—Ä–Ω–∞—è –¥–∞—Ç–∞"

def parse_page(company_url, depth):
    time.sleep(5)  # üî• –î–∞—Ç—å –æ—Ç–¥–æ—Ö–Ω—É—Ç—å IP –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
    response = requests.get(company_url, headers=get_headers(), timeout=10)
    if "–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –≤—ã —á–µ–ª–æ–≤–µ–∫" in response.text:
        print(f"[{depth}] üõë –ö–∞–ø—á–∞! –ü–æ–¥—Ç–≤–µ—Ä–¥–∏ –Ω–∞ —Å–∞–π—Ç–µ: {url}")
        input("üëâ –ù–∞–∂–º–∏ Enter, –∫–æ–≥–¥–∞ –∫–∞–ø—á–∞ –±—É–¥–µ—Ç –ø—Ä–æ–π–¥–µ–Ω–∞ –≤—Ä—É—á–Ω—É—é...")
        return parse_page(url, depth)  # –ø–æ–≤—Ç–æ—Ä—è–µ–º

    soup = BeautifulSoup(response.text, "html.parser")

    company_name = soup.select_one('h1#cn')
    company_name = company_name.get_text(strip=True) if company_name else ""
    print(f"[{depth}] {company_name} ‚Äî {company_url}")

    registration_date = soup.select_one(
        "#top > div > div.row.gy-2.gx-4 > div:nth-child(1) > div:nth-child(3) > div:nth-child(2)"
    )
    registration_date = format_date(registration_date.get_text(strip=True)) if registration_date else ""

    legal_address = soup.select_one("#copy-address")
    legal_address = legal_address.get_text(strip=True) if legal_address else ""

    contact_address = soup.select_one("#contacts > div:nth-child(3)")
    contact_address = contact_address.get_text(strip=True) if contact_address else ""

    phone_tags = soup.select("a.link-pseudo[href^='tel:']")
    first_phone  = phone_tags[0].get_text(strip=True) if len(phone_tags) > 0 else ""
    second_phone = phone_tags[1].get_text(strip=True) if len(phone_tags) > 1 else ""
    third_phone  = phone_tags[2].get_text(strip=True) if len(phone_tags) > 2 else ""

    email_tag = soup.select_one("a[href^='mailto:']")
    email = email_tag.get_text(strip=True) if email_tag else ""

    site = ""
    website_label = soup.find("strong", string="–í–µ–±-—Å–∞–π—Ç")
    if website_label:
        site_link = website_label.find_next_sibling("a")
        if site_link and site_link.has_attr("href"):
            site = site_link.get_text(strip=True)

    director_tag = soup.select_one("#management a")
    director = director_tag.get_text(strip=True) if director_tag else ""

    founders = []
    founders_table = soup.select_one("#founders-tab-1 > table")
    if founders_table:
        rows = founders_table.find_all("tr")
        for row in rows:
            cells = row.find_all("td")
            if len(cells) > 1:
                founders.append(cells[1].get_text(strip=True))
    founders = (founders + [""])[:1]

    activity = soup.select_one(
        "#top > div > div.row.gy-2.gx-4 > div:nth-child(1) > div:nth-child(4) > div:nth-child(2) > a"
    )
    activity = activity.get_text(strip=True) if activity else ""

    okved_code = soup.select_one('span.copy.ms-2.link-pseudo')
    okved_code = okved_code.get_text(strip=True) if okved_code else ""

    revenue_block = soup.select_one('a.link-black')
    if revenue_block:
        revenue_text = revenue_block.get_text(strip=True).replace("–º–ª–Ω —Ä—É–±.", "").strip()
        try:
            revenue = float(revenue_text.replace(",", "."))
        except:
            revenue = ""
    else:
        revenue = ""

    social_links = []
    social_block = soup.find("div", string="–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏")
    if social_block:
        parent = social_block.find_parent()
        if parent:
            for link in parent.find_all("a", href=True):
                social_links.append(link.get("href"))
    social = ", ".join(social_links) if social_links else ""

    data = [
        company_url, company_name, registration_date, legal_address, contact_address,
        first_phone, second_phone, third_phone,
        email, site, director, founders[0],
        activity, okved_code, revenue, social, depth
    ]

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
    competitors = []
    competitors_header = soup.find('h3', class_='header', string='–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã')
    if competitors_header:
        competitors_section = competitors_header.find_parent().find_all('a', class_='link')
        for link in competitors_section[:7]:
            href = "https://checko.ru" + link.get('href')
            competitors.append(href)

    return data, competitors

def crawl_company(url, depth, ws, headers_list): #–ì–ª—É–±–∏–Ω–∞ —Ä–µ–∫—É—Ä—Å–∏–∏
    if depth > 1 or url in parsed_urls:
        return

    try:
        data, competitors = parse_page(url, depth)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
        return

    ws.append(data)
    parsed_urls.add(url)
    time.sleep(random.uniform(3, 6))  # –∞–Ω—Ç–∏–±–∞–Ω

    for comp_url in competitors:
        crawl_company(comp_url, depth + 1, ws, headers_list)



def main():
    if len(sys.argv) > 1:
        ogrn = sys.argv[1].strip()
    else:
        ogrn = input("–í–≤–µ–¥–∏—Ç–µ –û–ì–†–ù –∏–ª–∏ –ò–ù–ù: ").strip()

    start_url = f"https://checko.ru/company/{ogrn}"

    headers_list = [
        "–°—Å—ã–ª–∫–∞", "–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏", "–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏", "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–¥—Ä–µ—Å",
        "–ê–¥—Ä–µ—Å", "–¢–µ–ª–µ—Ñ–æ–Ω 1", "–¢–µ–ª–µ—Ñ–æ–Ω 2", "–¢–µ–ª–µ—Ñ–æ–Ω 3",
        "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞", "–í–µ–±-—Å–∞–π—Ç", "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "–£—á—Ä–µ–¥–∏—Ç–µ–ª—å 1",
        "–í–∏–¥—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "–ö–æ–¥ –û–ö–í–≠–î", "–í—ã—Ä—É—á–∫–∞ –∑–∞ –≥–æ–¥ –ú–ª–Ω.—Ä—É–±", "–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏", "–ì–ª—É–±–∏–Ω–∞"
    ]

    wb = Workbook()
    ws = wb.active
    ws.append(headers_list)

    crawl_company(start_url, 0, ws, headers_list)

    filename = "ogrn_recursive_result.xlsx"
    wb.save(filename)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª: {filename}")

if __name__ == "__main__":
    main()
